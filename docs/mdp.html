

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Markov Decision Processes &mdash; Tic-Tac-Toe</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/styles.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hidden Markov Models" href="hmm.html" />
    <link rel="prev" title="TicTacToe-HMM" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> TicTacToe HMM
          

          
            
            <img src="_static/logo-white.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                2019.08
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Markov Decision Processes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#markov-chain">Markov Chain</a></li>
<li class="toctree-l2"><a class="reference internal" href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#rewards-and-returns">Rewards and returns</a></li>
<li class="toctree-l2"><a class="reference internal" href="#episodic-vs-continuous-tasks">Episodic VS Continuous Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#policy-function">Policy Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#state-value-function">State Value Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="#state-action-value-function-q-function">State-action Value function (Q function)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bellman-equation">Bellman equation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#value-iteration">Value Iteration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#policy-iteration">Policy Iteration</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="hmm.html">Hidden Markov Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">TicTacToe HMM</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Markov Decision Processes</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/mdp.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="hmm.html" class="btn btn-neutral float-right" title="Hidden Markov Models" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="TicTacToe-HMM" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="markov-decision-processes">
<span id="mdp"></span><h1>Markov Decision Processes<a class="headerlink" href="#markov-decision-processes" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>In RL, we have an agent that interacts with it’s environment. At each time step, the agent will get some representation
of the environment’s. Then, the agent selects an action to take. The environment is then transitioned into a new state
and the agent is given a reward as a consequence of the previous action.</p>
<p>Any RL task can be defined with the following components:</p>
<ul class="simple">
<li><p>Agent</p></li>
<li><p>States</p></li>
<li><p>Actions</p></li>
<li><p>Rewards</p></li>
</ul>
</div>
<div class="section" id="markov-chain">
<h2>Markov Chain<a class="headerlink" href="#markov-chain" title="Permalink to this headline">¶</a></h2>
<p>Markov’s property states that the future depends only on the present, not on the past. A Markov chain is a probabilistic
model that represent this kind of approach. Moving from one state to another is called transition and its probability is
called transition probability. We can represent a Markov chain as a state diagram:</p>
<a class="reference internal image-reference" href="_images/markov_chain.png"><img alt="_images/markov_chain.png" class="align-center" src="_images/markov_chain.png" style="width: 30%;" /></a>
</div>
<div class="section" id="markov-decision-process-mdp">
<h2>Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permalink to this headline">¶</a></h2>
<p>When an stochastic process is called follows Markov’s property, it is called a Markov Process. MDP is an extension of
the Markov chain. It provides a mathematical framework for modeling decision-making.</p>
<p>A MDP is completely defined with 4 elements:</p>
<ul class="simple">
<li><p>A set of states(<span class="math notranslate nohighlight">\(S\)</span>) the agent can be in.</p></li>
<li><p>A set of actions (<span class="math notranslate nohighlight">\(A\)</span>) that can be performed by an agent to move from one state to another.</p></li>
<li><p>A set of transition probabilities (<span class="math notranslate nohighlight">\(P^a_{ss'}\)</span>), which define the probability of moving from state <span class="math notranslate nohighlight">\(s\)</span> to state <span class="math notranslate nohighlight">\(s'\)</span> by performing action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
<li><p>A set of reward probabilities (<span class="math notranslate nohighlight">\(R^a_{ss'}\)</span>), which defines the probability of a reward acquired for moving from state <span class="math notranslate nohighlight">\(s\)</span> to state <span class="math notranslate nohighlight">\(s'\)</span> by performing action <span class="math notranslate nohighlight">\(a\)</span>.</p></li>
</ul>
</div>
<div class="section" id="rewards-and-returns">
<h2>Rewards and returns<a class="headerlink" href="#rewards-and-returns" title="Permalink to this headline">¶</a></h2>
<p>At each time step <span class="math notranslate nohighlight">\(t=0,1,2,...\)</span>, the agent receives some representation of the environment’s state
<span class="math notranslate nohighlight">\(s_t \in S\)</span>. Based on this state, the agent selects an action <span class="math notranslate nohighlight">\(a_t \in A\)</span>. The agent’s goal is to maximize
the total amount of rewards instead of immediate rewards. The total reward can be formulated as:
<span class="math notranslate nohighlight">\(R=r_{t+1}+r_{t+1}+\dots+r_{T}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(r_{t+i}\)</span> is the reward received at time step <span class="math notranslate nohighlight">\(t_{i-1}\)</span> while performing an action <span class="math notranslate nohighlight">\(a_{i-1}\)</span> to move
from state <span class="math notranslate nohighlight">\(s_{i-1}\)</span> to state <span class="math notranslate nohighlight">\(s_{i}\)</span>.</p>
</div>
<div class="section" id="episodic-vs-continuous-tasks">
<h2>Episodic VS Continuous Tasks<a class="headerlink" href="#episodic-vs-continuous-tasks" title="Permalink to this headline">¶</a></h2>
<p>Episodic tasks are those that have a terminal state at time <span class="math notranslate nohighlight">\(T\)</span>, which is followed by resetting the environment to
its initial state. The next episode then begins independently from how the previous episode ended. TicTacToe is an
episodic task, being each episode a round.</p>
<p>In a continuous task, there is not a terminal state. This kind of tasks will never end.</p>
</div>
<div class="section" id="policy-function">
<h2>Policy Function<a class="headerlink" href="#policy-function" title="Permalink to this headline">¶</a></h2>
<p>A policy function <span class="math notranslate nohighlight">\(\pi\)</span> maps the states to actions. It can be represented as <span class="math notranslate nohighlight">\(\pi(s): S \rightarrow A\)</span>.
Basically, a policy function says what action to perform in each state. The ultimate goal is finding the optimal policy
that results in the correct action to perform in each state maximizing the reward.</p>
</div>
<div class="section" id="state-value-function">
<h2>State Value Function<a class="headerlink" href="#state-value-function" title="Permalink to this headline">¶</a></h2>
<p>A state value function (or simply value function) specifies how good it is for an agent to be in a particular state with a
policy <span class="math notranslate nohighlight">\(\pi\)</span>. The value functions is defined as
<span class="math notranslate nohighlight">\(V^\pi(s)=\mathbb{E}_\pi [R_t|s_t=s]=\mathbb{E}_\pi [\sum_{k=1}^{T}r_{t+k}|s_t=s]\)</span>. This is the expected return
starting from state <span class="math notranslate nohighlight">\(s\)</span> according to policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>Based on the value of each state, we can tell how good it is for our agent to be in each one.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>State 1</p></td>
<td><p>0.7</p></td>
</tr>
<tr class="row-odd"><td><p>State 2</p></td>
<td><p>0.5</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="state-action-value-function-q-function">
<h2>State-action Value function (Q function)<a class="headerlink" href="#state-action-value-function-q-function" title="Permalink to this headline">¶</a></h2>
<p>A state-action value function is also called the Q function. It specifies how good it is for an agent to perform a
particular action in a state with a policy <span class="math notranslate nohighlight">\(\pi\)</span>. The Q function is defined as</p>
<p><span class="math notranslate nohighlight">\(Q^\pi(s,a)=\mathbb{E}_\pi [R_t|s_t=s]=\mathbb{E}_\pi [\sum_{k=1}^{T}r_{t+k}|s_t=s,a_t=a]\)</span></p>
<p>This function specifies the expected return starting from state <span class="math notranslate nohighlight">\(s\)</span> performing action <span class="math notranslate nohighlight">\(a\)</span> according to policy
<span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>State</p></th>
<th class="head"><p>Action</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>State 1</p></td>
<td><p>Action 1</p></td>
<td><p>0.7</p></td>
</tr>
<tr class="row-odd"><td><p>State 1</p></td>
<td><p>Action 2</p></td>
<td><p>0.01</p></td>
</tr>
<tr class="row-even"><td><p>State 2</p></td>
<td><p>Action 1</p></td>
<td><p>0.5</p></td>
</tr>
<tr class="row-odd"><td><p>State 2</p></td>
<td><p>Action 2</p></td>
<td><p>0.8</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The difference between the value function and the Q function is that the first one specifies the goodness of a
state and the latter specifies the goodness of an action in a state.</p>
</div>
</div>
<div class="section" id="bellman-equation">
<h2>Bellman equation<a class="headerlink" href="#bellman-equation" title="Permalink to this headline">¶</a></h2>
<p>The Bellman equation helps us finding the optimal policies and value functions. The optimal value function is the one
yielding maximum value compared to all other value functions. Similarly, the optimal policy is the one which results in
an optimal value function.</p>
<p>Since the optimal value function is the one that has a higher value, it will be the maximum ot the Q functions:
<span class="math notranslate nohighlight">\(V^*(s)=max_aQ^*(s,a)\)</span></p>
<p>The Bellman equation for the value and Q functions is:</p>
<p><span class="math notranslate nohighlight">\(V^\pi(s)=\sum_{a}\pi(s,a)\sum_{s'}P^a_{ss'}[R^a_{ss'}+\gamma V^\pi(s')]\)</span></p>
<p><span class="math notranslate nohighlight">\(Q^\pi(s,a)=\sum_{s'}P^a_{ss'}[R^a_{ss'}+\gamma\sum_{a'}Q^\pi(s',a')]\)</span></p>
<p>Finally, the Bellman optimality equation is:</p>
<p><span class="math notranslate nohighlight">\(V^*(s)=max_a\sum_{s'}P^a_{ss'}[R^a_{ss'}+\gamma\sum_{a'}Q^\pi(s',a')]\)</span></p>
<p>To solve this equation, two algorithms are used:</p>
<ul class="simple">
<li><p>Value iteration.</p></li>
<li><p>Policy iteration.</p></li>
</ul>
</div>
<div class="section" id="value-iteration">
<h2>Value Iteration<a class="headerlink" href="#value-iteration" title="Permalink to this headline">¶</a></h2>
<p>The steps involved in the value iteration are as follows:</p>
<ol class="arabic simple">
<li><p>We initialize the value function randomly.</p></li>
<li><p>Then we compute the Q function for all state-action pairs of <span class="math notranslate nohighlight">\(Q(s,a)\)</span>.</p></li>
<li><p>Then we update our value function with the max value from <span class="math notranslate nohighlight">\(Q(s,a)\)</span>.</p></li>
<li><p>We repeat these steps until the change in the value function is very small.</p></li>
</ol>
</div>
<div class="section" id="policy-iteration">
<h2>Policy Iteration<a class="headerlink" href="#policy-iteration" title="Permalink to this headline">¶</a></h2>
<p>We start with the random policy, then we find the value function of that policy; if the value function is not optimal,
then we find the new improved policy. We repeat this process till we find the optimal policy. There are two steps in
policy iteration:</p>
<ol class="arabic simple">
<li><p>Policy evaluation</p></li>
<li><p>Policy improvement</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="hmm.html" class="btn btn-neutral float-right" title="Hidden Markov Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="TicTacToe-HMM" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Sergio Bugallo

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>